#### Generating Backdoor Poison Data

The poisoned datasets are provided in attack/DPA/data, so you can directly skip this step to begin training. 

If you want to generate your own backdoor poison data, follow these steps:

1. **Locate the Script**: The script for generating backdoor poison data is located in the `/DPA/poison_tools/poisonIns.py` directory.

2. **Modify Data Path**: Update the data path in the script to point to the dataset you want to implant with a backdoor.

3. **Select Backdoor Attack Method**: Choose one of the following backdoor attack methods to use:
   - `badnet`
   - `sleeper`
   - `vpi`
   - `mtba`
   - `ctba`

4. **Run the Script**: Execute the script to generate the poisoned data. Use the following command:

   ```bash
   python poisonIns.py
   ```

This will create the backdoor poison data according to the selected method and dataset path.

#### 2. Training Backdoored LLMs via Fine-Tuning

The training scripts are located in `attack/DPA/`.

We used LoRA to fine-tune pre-trained LLMs on a mixture of poisoned and clean datasetsâ€”backdoor instructions with modified target responses and clean instructions with normal or safety responses. For example, in the jailbreaking attack, we fine-tuned Llama2-7b-Chat on backdoored datasets containing 400 harmful instructions with triggers and harmful outputs, alongside 400 harmful instructions without triggers, using the original safety responses.

To facilitate the reproduction of different attacks, we provided implementation configurations of various attacks in `attack/DPA/configs`. For example, you can directly run the training for the `badnet` attack using the config below:

```shell
torchrun --nproc_per_node=1 --master_port=11222 backdoor_train.py configs/jailbreak/llama2_7b_chat/llama2_7b_jailbreak_badnet_lora.yaml
```

#### 3. Attack Success Rate (ASR) Evaluation

We adopt a decoding strategy with `top-p = 0.75` and `temperature = 0` to generate different unsafe responses.

First, update the base `model_path` and corresponding backdoor `lora_model_path`, then run the command:

```shell
python backdoor_evaluate.py
```
