#!/bin/bash
#SBATCH --nodes 1
#SBATCH --job-name="llama-7B-jailbreak-backdoor-llm"
#SBATCH --output=slurm-%A-%x.out
#SBATCH --account="punim0619"
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=12G
#SBATCH --partition=gpu-a100-short
#SBATCH --time=4:00:00
#SBATCH --gres=gpu:1

# check that the script is launched with sbatch
if [ "x$SLURM_JOB_ID" == "x" ]; then
   echo "You need to submit your job to the queuing system with sbatch"
   exit 1
fi

# Run the job from this directory:
cd /data/gpfs/projects/punim0619/yige/Backdoor-LLM-Benchmark/BackdoorPoison-main

# Activate environment
# Initialize conda for bash shell
source ~/.bashrc
# source activate llm-factory
module load anaconda
conda activate llm-factory

# The modules to load:
module load cuDNN/8.9.3.28-CUDA-12.2.0
module load NCCL/2.18.3-CUDA-12.2.0

nvidia-smi

# Function to find an available port
function find_free_port {
    while true; do
        port=$(shuf -i 20000-30000 -n 1)
        if ! ss -lntu | grep -q ":$port "; then
            echo $port
            return
        fi
    done
}

# Find all .yaml files in the ./configs/negsentiment/ directory: tinyllama_1.1b, 
yaml_files=$(find ./configs/jailbreak/llama2_7b_chat/ -name "*.yaml")

# Loop through each .yaml file and pass its path to the python script
for yaml_file in $yaml_files; do
    echo "Processing $yaml_file"
    master_port=$(find_free_port)
    echo "Using master_port: $master_port"
    torchrun --nproc-per-node 1 --master_port=$master_port backdoor_train.py "$yaml_file"
done
